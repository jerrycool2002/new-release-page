<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <!-- <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/images/carousel2.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> --> 


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <!-- <meta name="keywords" content="LLMs, Adaptation">
  <meta name="viewport" content="width=device-width, initial-scale=1"> --> 


  <title>EHRAgent</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title" style="font-size: 2.9em; font-weight: bold;"><img src="static/images/cartoon.png" alt="Alt text" style="height: 1.5em; vertical-align: middle;" />EHRAgent<br><span style="font-size: 0.8em; font-weight: normal;"> Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records</span></h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://wshi83.github.io/" target="_blank">Wenqi Shi</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://ritaranx.github.io/" target="_blank">Ran Xu</a><sup>2*</sup>,</span>
              <span class="author-block">
                <a href="https://night-chen.github.io" target="_blank">Yuchen Zhuang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://yueyu1030.github.io/" target="_blank">Yue Yu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://jieyuz2.github.io/" target="_blank">Jieyu Zhang</a><sup>3</sup>,</span>      
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=3gVKIOwAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Hang Wu</a><sup>1</sup>,</span>      
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=uYPzS5MAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Yuanda Zhu</a><sup>1</sup>,</span>     
              <span class="author-block">
                <a href="https://joyceho.github.io/" target="_blank">Joyce C. Ho</a><sup>2</sup>,</span>     
              <span class="author-block">
                <a href="https://cs.emory.edu/~jyang71/" target="_blank">Carl Yang</a><sup>2</sup>,</span>     
              <span class="author-block">
                <a href="https://bme.gatech.edu/bme/faculty/May-Dongmei-Wang" target="_blank">May D. Wang</a><sup>1</sup>
                  </span>
                  
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Georgia Institute of Technology,</span>
                    <span class="author-block"><sup>2</sup>Emory University,</span>
                    <span class="author-block"><sup>3</sup>University of Washington</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2401.07128.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/wshi83/EhrAgent" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2401.07128" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <!-- Your video here -->
        <img src="static/images/EHRAgent-video.gif" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        EHRAgent is an autonomous LLM agent with external tools and code interface for improved multi-tabular reasoning across EHRs.       </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have demonstrated exceptional capabilities in planning and tool utilization as autonomous agents, but few have been developed for medical problem-solving. We propose EHRAgent, an LLM agent empowered with a code interface, to autonomously generate and execute code for multi-tabular reasoning within electronic health records (EHRs). First, we formulate an EHR question-answering task into a tool-use planning process, efficiently decomposing a complicated task into a sequence of manageable actions. By integrating interactive coding and execution feedback, EHRAgent learns from error messages and improves the originally generated code through iterations. Furthermore, we enhance the LLM agent by incorporating long-term memory, which allows EHRAgent to effectively select and build upon the most relevant successful cases from past experiences. Experiments on three real-world multi-tabular EHR datasets show that EHRAgent outperforms the strongest baseline by up to 29.6% in success rate. EHRAgent leverages the emerging few-shot learning capabilities of LLMs, enabling autonomous code generation and execution to tackle complex clinical tasks with minimal demonstrations.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section" id="Examples">
  <div class="container is-max-desktop content">
    <h2 class="title">Examples</h2>
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Dropdown Example</title>
      <style>
        .dropdown-container {
          margin: 20px;
          padding: 10px;
        }
        select {
          width: 200px;
          padding: 5px;
          margin-right: 10px;
        }
        .figure img {
          max-width: 100%;
          height: auto;
        }

        .visible {
          display: block;
        }
      </style>
      </head>
      <body>
      
      <div class="dropdown-container">
        <label for="exampleSelector">Select the example:</label>
        <br>
        <select id="exampleSelector" name="exampleSelector">
          <option value="">--Select--</option>
          <option value="example0">Example 0: MIMIC-III</option>
          <option value="example1">Example 1: MIMIC-III</option>
          <option value="example2">Example 2: eICU</option>
          <option value="example3">Example 3: eICU</option>
          <option value="example4">Example 4: TREQS</option>
          <option value="example5">Example 5: TREQS</option>
          <!-- Add other options here -->
        </select>
      </div>
      <div id="example0" class="figure" style="display:none;">
        <img src="static/images/web-mimiciii-case1.gif" alt="Image 1">
      </div>
      <div id="example1" class="figure" style="display:none;">
        <img src="static/images/web-mimiciii-case2.gif" alt="Image 2">
      </div>
      <div id="example2" class="figure" style="display:none;">
        <img src="static/images/web-eicu-case3.gif" alt="Image 3">
      </div>
      <div id="example3" class="figure" style="display:none;">
        <img src="static/images/web-eicu-case4.gif" alt="Image 4">
      </div>
      <div id="example4" class="figure" style="display:none;">
        <img src="static/images/web-treqs-case5.gif" alt="Image 5">
      </div>
      <div id="example5" class="figure" style="display:none;">
        <img src="static/images/web-treqs-case6.gif" alt="Image 6">
      </div>
    </div>
    <script>
      document.getElementById('figureSelector').addEventListener('change', function() {
          // Hide all figures first
          document.querySelectorAll('.figure').forEach(function(figure) {
              figure.style.display = 'none';
          });

          // Show the selected figure
          var selectedFigure = document.getElementById(this.value);
          if (selectedFigure) {
              selectedFigure.style.display = 'block';
          }
      });
  </script>
      </body>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Introduction</h2>
    Adapting black-box LLMs through fine-tuning APIs has several critical issues on <b>transparency</b>, <b>privacy</b>, and <b>cost</b>. The adaptation of black-box LLMs without the use of APIs remains an unresolved challenge.
    <br><br>
    <div style="display: block; justify-content: center;">
      <img src="static/images/teaser.png" alt="MY ALT TEXT"width="450" height="310">
  </div>
    <br>
    Due to the black-box nature, users are unable to access
    <ul>
      <li>internal model parameters,</li>
      <li>high-dimensional representations of input sequences or output generations, and</li>
      <li>output token probabilities for their specific use cases in black-box adaptation.</li>
    </ul>
    Notably, existing methods, except ours, fail to support black-box LLM adaptations, where neither model parameters nor output probabilities can be accessed in most recent LLMs like GPT-3.5 and Gemini. BBox-Adapter adopts an online adaptation framework, iteratively sampling from previous inferences and updating the adapter.
    <br><br>
    <img src="static/images/overview.png" alt="MY ALT TEXT">
    <br>
    <h2 class="title">Experiments</h2>
    <br>
    We evaluate BBox-Adapter on four distinct question-answering tasks, requiring model adaptation on mathematical (GSM8K), implicit-reasoning (StrategyQA), truthful (TruthfulQA), and scientific (ScienceQA) domains.     <ul>
    <h3 class="title">Main Results</h3>
    <img src="static/images/main_table.png" alt="MY ALT TEXT">
    <br>
    BBox-Adapter consistently outperforms gpt-3.5-turbo by an average of 6.39% across all datasets, highlighting its efficacy in adapting black-box LLMs to specific tasks.
    Notably, BBox-Adapter (AI Feedback) demonstrates competitive performance compared to BBox-Adapter (Ground-Truth), which demonstrates its robust generalization capability across datasets, even in the absence of ground-truth answers.
    Furthermore, BBox-Adapter (Combined) achieves the highest performance among the three variations.
    This enhanced performance can be attributed to the combination of high-quality initial positive sets derived from ground-truth solutions and the dynamic updating of positive sets through AI feedback, leading to the continuous self-improvement of BBox-Adapter. 
    <h3 class="title">Plug-and-Play</h3>
    <br>
    <img src="static/images/pnp.png" alt="MY ALT TEXT">
    <br>
    The tuned BBox-Adapter can be seamlessly applied to various black-box LLMs in a plug-and-play manner, eliminating the need for retraining or additional technical modifications. Compared to their unadapted black-box LLMs, davinci-002 and Mixtral-8x7B, our trained adapter demonstrates an average performance improvement of 6.85% and 4.50% across all three datasets, respectively. 
    The effectiveness of BBox-Adapter in plug-and-play scenarios arises from its independence from the internal parameters of black-box LLMs. 
    <h3 class="title">Costs</h3>
    <br>
    <img src="static/images/cost.png" alt="MY ALT TEXT">
    <br>
    Compared with the base model, Azure-SFT boosts accuracy by an average of 6.35% at the expense of significantly higher costs.
BBox-Adapter, in single-step inference variant, brings 3.45% performance gain compared with the base model, with 41.97 times less training cost and 6.27 times less inference cost than SFT.
Meanwhile, its full-step inference variant achieves 5.90% improvement over the base model with 31.30 times less training cost and 1.84 times less inference cost.
This increased cost in its full-step variant is attributed to the integration of a beam search in the adapted inference, which requires the use of the black-box LLM APIs to generate multiple solution paths for selection.

    <h3 class="title">Case Study</h3>
    <br>Here is a case study of BBox-Adapter on GSM8K. For the given question, the CoT solution from original gpt-3.5-turbo is incorrect, while the model adapted using BBox-Adapter successfully executed a logical, step-by-step search, ultimately yielding the correct answer. For clarity, we display only the top-3 candidate answers at each step.
    <br><br>
    <img src="static/images/case.png" alt="MY ALT TEXT">

  </div>
</section>












<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{sun2024bboxadapter,
        title={BBox-Adapter: Lightweight Adapting for Black-Box Large Language Models}, 
        author={Haotian Sun and Yuchen Zhuang and Wei Wei and Chao Zhang and Bo Dai},
        year={2024},
        eprint={2402.08219},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


 
  </body>
  </html>
